{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Regression Model\n",
    "\n",
    "**Student Name:** Sean McManus\n",
    "\n",
    "**Dataset:** Global_Space_Exploration_Dataset.csv\n",
    "\n",
    "**Target Variable:** success_rate\n",
    "\n",
    "**Checkpoint 3 Due:** Feb 15\n",
    "\n",
    "---\n",
    "\n",
    "## Rules & Integrity\n",
    "\n",
    "1. **NO AI TOOLS**: You may **NOT** use ChatGPT, Claude, Gemini, GitHub Copilot, or any other AI assistant to generate code for this assignment. The goal is to build *your* fundamental skills. If you rely on AI now, the advanced topics later will be impossible.\n",
    "\n",
    "2. **Study Groups Encouraged**: You **ARE** encouraged to discuss ideas, share approaches, and explain concepts to your study group peers. Teaching others is the best way to learn! However, the code you submit must be **your own work**.\n",
    "\n",
    "3. **Use Your Resources**: You are free to use Google, StackOverflow, Pandas/Scikit-learn documentation, and your class notes.\n",
    "\n",
    "4. **Comment Your Code**: Include comments explaining *why* you're doing what you're doing. I want to see your thought process.\n",
    "\n",
    "5. **Resubmission**: You may submit this assignment multiple times for feedback before the checkpoint deadline.\n",
    "\n",
    "---\n",
    "\n",
    "## Important: Written Reflections\n",
    "\n",
    "Throughout this notebook, you'll be asked to interpret results, justify decisions, and explain your reasoning. **These written reflections are a critical part of your grade.**\n",
    "\n",
    "Good data scientists don't just run code—they communicate their findings clearly. Take time to write thoughtful, complete responses to all reflection prompts. This demonstrates your understanding and prepares you for real-world stakeholder communication.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn - preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sklearn - models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Sklearn - evaluation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Model saving\n",
    "import joblib\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (3000, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Budget (in Billion $)</th>\n",
       "      <th>Success Rate (%)</th>\n",
       "      <th>Duration (in Days)</th>\n",
       "      <th>Num_Collaborators</th>\n",
       "      <th>Tech_Maturity</th>\n",
       "      <th>Budget_Per_Day</th>\n",
       "      <th>Mission Type_Unmanned</th>\n",
       "      <th>Technology Used_Nuclear Propulsion</th>\n",
       "      <th>Technology Used_Reusable Rocket</th>\n",
       "      <th>...</th>\n",
       "      <th>Country_UAE</th>\n",
       "      <th>Country_UK</th>\n",
       "      <th>Country_USA</th>\n",
       "      <th>Satellite Type_Navigation</th>\n",
       "      <th>Satellite Type_Research</th>\n",
       "      <th>Satellite Type_Spy</th>\n",
       "      <th>Satellite Type_Weather</th>\n",
       "      <th>Environmental Impact_Low</th>\n",
       "      <th>Environmental Impact_Medium</th>\n",
       "      <th>Tech_Budget_Power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>16.20</td>\n",
       "      <td>90.0</td>\n",
       "      <td>112</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.143363</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>48.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>29.04</td>\n",
       "      <td>99.0</td>\n",
       "      <td>236</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.122532</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>58.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>28.73</td>\n",
       "      <td>54.0</td>\n",
       "      <td>238</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.120209</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>28.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010</td>\n",
       "      <td>37.27</td>\n",
       "      <td>58.0</td>\n",
       "      <td>186</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.199305</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>37.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006</td>\n",
       "      <td>18.95</td>\n",
       "      <td>91.0</td>\n",
       "      <td>277</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.068165</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>37.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Budget (in Billion $)  Success Rate (%)  Duration (in Days)  \\\n",
       "0  2008                  16.20              90.0                 112   \n",
       "1  2018                  29.04              99.0                 236   \n",
       "2  2013                  28.73              54.0                 238   \n",
       "3  2010                  37.27              58.0                 186   \n",
       "4  2006                  18.95              91.0                 277   \n",
       "\n",
       "   Num_Collaborators  Tech_Maturity  Budget_Per_Day  Mission Type_Unmanned  \\\n",
       "0                  3            3.0        0.143363                  False   \n",
       "1                  2            2.0        0.122532                  False   \n",
       "2                  3            1.0        0.120209                  False   \n",
       "3                  1            1.0        0.199305                   True   \n",
       "4                  3            2.0        0.068165                  False   \n",
       "\n",
       "   Technology Used_Nuclear Propulsion  Technology Used_Reusable Rocket  ...  \\\n",
       "0                                True                            False  ...   \n",
       "1                               False                            False  ...   \n",
       "2                               False                            False  ...   \n",
       "3                               False                            False  ...   \n",
       "4                               False                            False  ...   \n",
       "\n",
       "   Country_UAE  Country_UK  Country_USA  Satellite Type_Navigation  \\\n",
       "0        False       False        False                      False   \n",
       "1        False       False        False                      False   \n",
       "2        False       False        False                      False   \n",
       "3         True       False        False                      False   \n",
       "4        False       False        False                      False   \n",
       "\n",
       "   Satellite Type_Research  Satellite Type_Spy  Satellite Type_Weather  \\\n",
       "0                    False               False                   False   \n",
       "1                    False               False                   False   \n",
       "2                    False               False                   False   \n",
       "3                    False                True                   False   \n",
       "4                    False               False                    True   \n",
       "\n",
       "   Environmental Impact_Low  Environmental Impact_Medium  Tech_Budget_Power  \n",
       "0                     False                         True              48.60  \n",
       "1                     False                        False              58.08  \n",
       "2                     False                         True              28.73  \n",
       "3                      True                        False              37.27  \n",
       "4                     False                         True              37.90  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load cleaned data from previous notebook\n",
    "df = pd.read_csv(\"Global_Space_Exploration_Dataset.csv\")\n",
    "\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Year', 'Budget (in Billion $)', 'Success Rate (%)', 'Duration (in Days)', 'Num_Collaborators', 'Tech_Maturity', 'Budget_Per_Day', 'Mission Type_Unmanned', 'Technology Used_Nuclear Propulsion', 'Technology Used_Reusable Rocket', 'Technology Used_Solar Propulsion', 'Technology Used_Traditional Rocket', 'Country_France', 'Country_Germany', 'Country_India', 'Country_Israel', 'Country_Japan', 'Country_Russia', 'Country_UAE', 'Country_UK', 'Country_USA', 'Satellite Type_Navigation', 'Satellite Type_Research', 'Satellite Type_Spy', 'Satellite Type_Weather', 'Environmental Impact_Low', 'Environmental Impact_Medium', 'Tech_Budget_Power']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist()) #Quick check to confirm name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: Success Rate (%)\n",
      "\n",
      "Target statistics:\n",
      "count    3000.000000\n",
      "mean       74.985000\n",
      "std        14.945252\n",
      "min        50.000000\n",
      "25%        62.000000\n",
      "50%        75.000000\n",
      "75%        88.000000\n",
      "max       100.000000\n",
      "Name: Success Rate (%), dtype: float64\n",
      "\n",
      "Target range: 50.00\n",
      "Target std: 14.95\n"
     ]
    }
   ],
   "source": [
    "# Define your target variable\n",
    "TARGET = 'Success Rate (%)'  # <-- UPDATE THIS!\n",
    "\n",
    "print(f\"Target: {TARGET}\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(df[TARGET].describe())\n",
    "\n",
    "# Store target range for later interpretation\n",
    "target_range = df[TARGET].max() - df[TARGET].min()\n",
    "target_std = df[TARGET].std()\n",
    "print(f\"\\nTarget range: {target_range:,.2f}\")\n",
    "print(f\"Target std: {target_std:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (3000, 27)\n",
      "Target shape: (3000,)\n",
      "\n",
      "All features (27):\n",
      "['Year', 'Budget (in Billion $)', 'Duration (in Days)', 'Num_Collaborators', 'Tech_Maturity', 'Budget_Per_Day', 'Mission Type_Unmanned', 'Technology Used_Nuclear Propulsion', 'Technology Used_Reusable Rocket', 'Technology Used_Solar Propulsion', 'Technology Used_Traditional Rocket', 'Country_France', 'Country_Germany', 'Country_India', 'Country_Israel', 'Country_Japan', 'Country_Russia', 'Country_UAE', 'Country_UK', 'Country_USA', 'Satellite Type_Navigation', 'Satellite Type_Research', 'Satellite Type_Spy', 'Satellite Type_Weather', 'Environmental Impact_Low', 'Environmental Impact_Medium', 'Tech_Budget_Power']\n"
     ]
    }
   ],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nAll features ({len(X.columns)}):\")\n",
    "print(X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features are numeric. Ready to proceed!\n"
     ]
    }
   ],
   "source": [
    "# Check for any non-numeric columns that need to be handled\n",
    "non_numeric = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "if non_numeric:\n",
    "    print(f\"Warning: Non-numeric columns found: {non_numeric}\")\n",
    "    print(\"You need to encode these or go back to Notebook 01!\")\n",
    "else:\n",
    "    print(\"All features are numeric. Ready to proceed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Train-Test Split\n",
    "\n",
    "Split your data into training and test sets. The training set is used to train the model, and the test set is used to evaluate how well it generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 2400 missions\n",
      "Testing set: 600 missions\n",
      "Total features: 27\n"
     ]
    }
   ],
   "source": [
    "# TODO: Split your data into training and test sets\n",
    "# \n",
    "# Requirements:\n",
    "# - Use an 80/20 split (test_size=0.2)\n",
    "# - Set random_state=42 for reproducibility # the best number\n",
    "# - Store results in: X_train, X_test, y_train, y_test\n",
    "\n",
    "# Hint: Use train_test_split(X, y, ...)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "# lest check this worked\n",
    "print(f\"Training set: {X_train.shape[0]} missions\")\n",
    "print(f\"Testing set: {X_test.shape[0]} missions\")\n",
    "print(f\"Total features: {X_train.shape[1]}\")\n",
    "\n",
    "#thanks David for that check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 2,400 samples (80%)\n",
      "Test set: 600 samples (20%)\n"
     ]
    }
   ],
   "source": [
    "# Verify your split (run this cell to check)\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Feature Scaling\n",
    "\n",
    "Many ML algorithms perform better when features are on similar scales. StandardScaler transforms features to have mean=0 and std=1.\n",
    "\n",
    "**Important:** Fit the scaler on training data only, then transform both train and test. This prevents data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scale your features using StandardScaler\n",
    "#\n",
    "# Steps:\n",
    "# 1. Create a StandardScaler instance #### Instance???  = object = the actual scaler i will use\n",
    "# 2. Fit the scaler on X_train and transform X_train (use fit_transform)  ### ok fit_transform will calculate mean and standard dev of X-train\n",
    "# 3. Transform X_test using the same scaler (use transform only - NOT fit_transform!)\n",
    "# 4. Store results in: X_train_scaled, X_test_scaled\n",
    "#\n",
    "# Why fit only on train? To prevent \"data leakage\" - test data should be truly unseen.\n",
    "# Its Scaler // not scalar\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)    ##PUT CORRECT SET IN SCALER FIT TRANSFORM***Not testing data X_TRAIN IMportant\n",
    "\n",
    "#!!from step 3!!! dont use fit_transform for test data \n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled successfully!\n",
      "\n",
      "Scaled feature means (should be ~0): -0.0000\n",
      "Scaled feature stds (should be ~1): 1.0002\n"
     ]
    }
   ],
   "source": [
    "# Convert back to DataFrame for easier viewing (optional but helpful)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# Verify scaling worked\n",
    "print(\"Features scaled successfully!\")\n",
    "print(f\"\\nScaled feature means (should be ~0): {X_train_scaled.mean().mean():.4f}\")\n",
    "print(f\"Scaled feature stds (should be ~1): {X_train_scaled.std().mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Baseline Model\n",
    "\n",
    "Start with a simple Linear Regression to establish a baseline performance. This gives us a reference point for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function evaluates any model - you'll use it throughout this notebook\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Train model and return evaluation metrics.\"\"\"\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Train R2': r2_score(y_train, y_train_pred),\n",
    "        'Test R2': r2_score(y_test, y_test_pred),\n",
    "        'Train RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        'Test RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "        'Train MAE': mean_absolute_error(y_train, y_train_pred),\n",
    "        'Test MAE': mean_absolute_error(y_test, y_test_pred)\n",
    "    }\n",
    "    \n",
    "    return results, model, y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and evaluate a baseline Linear Regression model\n",
    "#\n",
    "# Steps:\n",
    "# 1. Create a LinearRegression() model instance\n",
    "# 2. Use the evaluate_model() function to train and evaluate it\n",
    "# 3. Store the results  ### DICTIONARY\n",
    "#\n",
    "# The evaluate_model function returns: (results_dict, trained_model, predictions)\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# baseline_model = ...\n",
    "# baseline_results, baseline_trained, baseline_preds = evaluate_model(...)\n",
    "baseline_model = LinearRegression()\n",
    "#scaled data is what is necissary for the dictinary\n",
    "baseline_results, baseline_trained, baseline_preds = evaluate_model(\n",
    "    model = baseline_model,\n",
    "    X_train = X_train_scaled,  ##USE SCALED DATA\n",
    "    X_test = X_test_scaled,\n",
    "    y_train = y_train,\n",
    "    y_test = y_test,\n",
    "    model_name = \"Baseline Spaceline Linear Regression Model\"\n",
    ")\n",
    "#/// dictionary classification is case sensitive frm now on!! (*model mistake*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "BASELINE MODEL: Linear Regression\n",
      "==================================================\n",
      "Train R²: 0.0076\n",
      "Test R²:  -0.0096\n",
      "Test RMSE: 15.28\n",
      "Test MAE: 13.34\n",
      "\n",
      "--- RMSE in Context ---\n",
      "RMSE as % of target range: 30.6%\n",
      "RMSE as % of target std: 102.2%\n"
     ]
    }
   ],
   "source": [
    "# Display baseline results\n",
    "print(\"=\" * 50)\n",
    "print(\"BASELINE MODEL: Linear Regression\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Train R²: {baseline_results['Train R2']:.4f}\")\n",
    "print(f\"Test R²:  {baseline_results['Test R2']:.4f}\")\n",
    "print(f\"Test RMSE: {baseline_results['Test RMSE']:,.2f}\")\n",
    "print(f\"Test MAE: {baseline_results['Test MAE']:,.2f}\")\n",
    "\n",
    "# Context for RMSE\n",
    "print(f\"\\n--- RMSE in Context ---\")\n",
    "print(f\"RMSE as % of target range: {baseline_results['Test RMSE']/target_range*100:.1f}%\")\n",
    "print(f\"RMSE as % of target std: {baseline_results['Test RMSE']/target_std*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model         Baseline Spaceline Linear Regression Model\n",
      "Train R2                                        0.007616\n",
      "Test R2                                        -0.009644\n",
      "Train RMSE                                     14.819956\n",
      "Test RMSE                                      15.277089\n",
      "Train MAE                                      12.879073\n",
      "Test MAE                                       13.338825\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(baseline_results)) ## SUber neat way i can do this n the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Interpretation\n",
    "\n",
    "*Analyze your baseline results:*\n",
    "\n",
    "**Understanding your metrics:**\n",
    "- **R² (0 to 1)**: How much variance in target is explained by features. 0.7+ is often considered good. # :/\n",
    "- **RMSE**: Average prediction error in the same units as your target. Lower is better.\n",
    "- **RMSE as % of range**: Helps you understand if errors are big or small relative to your data.\n",
    "\n",
    "**Questions to answer:**\n",
    "- What does your R² score tell you about how well features explain the target?\n",
    "- Is there a big gap between train and test R²? (Gap > 0.1 could indicate overfitting)\n",
    "- Is your RMSE reasonable? (e.g., if predicting house prices, is a $20K error acceptable?)\n",
    "\n",
    "**Your interpretation:**\n",
    "\n",
    "-Ok so test R^2 of 0.0096 (about 0.01 )... thats worse than guessing the success rate of each mission.. and the train r^2 is 0.0076. So the features are explaining less that 1% of the variations in missions and success rates, and can be considered \"blind\".\n",
    "-There is no big gap. the train and test R^2 are both around zero. So the mode lisnt overfitting, but since these values are so low its Underfitting.. this is probably not be the model I should use.\n",
    "- the RMSE is 15.28 and my target variable is success rate of a dadgum Space mission worth billions of dollars, which is a percentage range from 0-100. 15% is a HUGE issue for a space mission. So not reasonable, definitely not for the kind of predictions we need to make; Expensive Predictions. Maybe even life dependant missions. \n",
    "\n",
    "I might have to trim this down. I have so many columns. i think it might have too many features. especially the One-hot encoded ones. if my missions are either (FAILURE) or (Success) theres not much middle ground to navigate, maybe making a regression model difficult for this situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Model Iteration\n",
    "\n",
    "Try at least 2-3 different models to see if you can improve on the baseline.\n",
    "\n",
    "lets go with ridge first for the noisy features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results for comparison\n",
    "all_results = [baseline_results]\n",
    "\n",
    "# Dictionary to store trained models\n",
    "trained_models = {\n",
    "    'Linear Regression (Baseline)': baseline_trained\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nridge_results, ridge_trained, ridge_preds = evaluate_model(\\n    Ridge(alpha=1.0), \\n    X_train_scaled, X_test_scaled, y_train, y_test, \\n    \"Ridge Regression\"\\n)\\nprint(pd.Series(ridge_results))\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ridge_results, ridge_trained, ridge_preds = evaluate_model(\n",
    "    Ridge(alpha=1.0), \n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, \n",
    "    \"Ridge Regression\"\n",
    ")\n",
    "print(pd.Series(ridge_results))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Ridge Regression\n",
    "\n",
    "Ridge adds L2 regularization to prevent overfitting by penalizing large coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression - Test R²: -0.0091, Test RMSE: 15.27\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create and evaluate a Ridge Regression model\n",
    "#\n",
    "# Create a Ridge model with alpha=1.0\n",
    "# Use evaluate_model() to train and evaluate\n",
    "# Add results to all_results list\n",
    "# Add trained model to trained_models dict\n",
    "\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_results, ridge_trained, ridge_preds = evaluate_model(\n",
    "    model = ridge_model,\n",
    "    X_train = X_train_scaled,\n",
    "    X_test = X_test_scaled,\n",
    "    y_train = y_train,\n",
    "    y_test = y_test,\n",
    "    model_name = \"Ridg-E-Mod Ridge Regression Model\"\n",
    "\n",
    ")\n",
    "#have to add the trained model results to the model list in the secsion before this one.\n",
    "all_results.append(ridge_results)\n",
    "trained_models[\"Ridge Regression\"] = ridge_trained\n",
    "\n",
    "# Print results\n",
    "print(f\"Ridge Regression - Test R²: {ridge_results['Test R2']:.4f}, Test RMSE: {ridge_results['Test RMSE']:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model         Ridg-E-Mod Ridge Regression Model\n",
      "Train R2                               0.007629\n",
      "Test R2                               -0.009128\n",
      "Train RMSE                            14.819859\n",
      "Test RMSE                             15.273187\n",
      "Train MAE                             12.879627\n",
      "Test MAE                               13.33516\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# oh lord.. lets get it all\n",
    "print(pd.Series(ridge_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Lasso Regression\n",
    "\n",
    "Lasso adds L1 regularization, which can zero out unimportant features (automatic feature selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression - Test R²: -0.0039, Test RMSE: 15.23\n",
      "\n",
      "Lasso kept 18 of 27 features\n",
      "Kept features:\n",
      "Technology Used_Nuclear Propulsion   -0.230719\n",
      "Budget_Per_Day                       -0.165288\n",
      "Satellite Type_Spy                   -0.161222\n",
      "Mission Type_Unmanned                -0.128107\n",
      "Duration (in Days)                   -0.099899\n",
      "Country_USA                          -0.089839\n",
      "Country_Israel                       -0.026457\n",
      "Country_Russia                        0.040910\n",
      "Tech_Budget_Power                     0.071931\n",
      "Num_Collaborators                     0.114028\n",
      "Year                                  0.155417\n",
      "Satellite Type_Research               0.157598\n",
      "Environmental Impact_Low              0.158041\n",
      "Satellite Type_Navigation             0.160926\n",
      "Country_India                         0.225194\n",
      "Country_Germany                       0.245694\n",
      "Country_France                        0.246360\n",
      "Technology Used_Reusable Rocket       0.534895\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create and evaluate a Lasso Regression model\n",
    "#\n",
    "# Create a Lasso model with alpha=0.1\n",
    "# Use evaluate_model() to train and evaluate\n",
    "# Add results to all_results and trained_models\n",
    "\n",
    "lasso_model = Lasso(alpha=0.1)  ### 0.1 // not 1.0 that was for the ridge\n",
    "\n",
    "lasso_results, lasso_trained, lasso_preds = evaluate_model(\n",
    "    model = lasso_model,\n",
    "    X_train = X_train_scaled,\n",
    "    X_test = X_test_scaled,\n",
    "    y_train = y_train,\n",
    "    y_test = y_test,\n",
    "    model_name = \"Lasso Regression Yee-Haw\"\n",
    "\n",
    ")\n",
    "\n",
    "all_results.append(lasso_results)\n",
    "trained_models[\"Lasso Regression\"] = lasso_trained\n",
    "# Print results\n",
    "print(f\"Lasso Regression - Test R²: {lasso_results['Test R2']:.4f}, Test RMSE: {lasso_results['Test RMSE']:,.2f}\")\n",
    "\n",
    "# Show which features Lasso kept (non-zero coefficients)\n",
    "lasso_coefs = pd.Series(lasso_trained.coef_, index=X.columns)\n",
    "kept_features = lasso_coefs[lasso_coefs != 0]\n",
    "print(f\"\\nLasso kept {len(kept_features)} of {len(X.columns)} features\")\n",
    "print(\"Kept features:\")\n",
    "print(kept_features.sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " YOOOOO? Reuseable Rocket looking really good really putting my hopes on the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model         Lasso Regression Yee-Haw\n",
      "Train R2                      0.006238\n",
      "Test R2                      -0.003939\n",
      "Train RMSE                    14.83024\n",
      "Test RMSE                    15.233868\n",
      "Train MAE                    12.895708\n",
      "Test MAE                      13.31361\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(lasso_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Test R²: -0.4060, Test RMSE: 18.03\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create and evaluate a Decision Tree model\n",
    "#\n",
    "# Create a DecisionTreeRegressor with max_depth=10 and random_state=42\n",
    "# Use evaluate_model() to train and evaluate\n",
    "# Add results to all_results and trained_models\n",
    "\n",
    "tree_model = DecisionTreeRegressor(max_depth=10, random_state=42) #gotta mage sure to get the case sensitive // could not tell where i went wrong\n",
    "\n",
    "tree_results, tree_trained, tree_preds = evaluate_model(\n",
    "    model = tree_model,\n",
    "    X_train = X_train_scaled,\n",
    "    X_test = X_test_scaled,\n",
    "    y_train = y_train,\n",
    "    y_test = y_test,\n",
    "    model_name = \"Decission-Maker Regression Arts: 「Yggdrasil」\"  \n",
    ")\n",
    "all_results.append(tree_results)\n",
    "trained_models[\"Decission-Maker Regression Arts: 「Yggdrasil」\"] = tree_trained\n",
    "# Print results /// woops gotta change this to match the model name in the dictinary\n",
    "print(f\"Decision Tree - Test R²: {tree_results['Test R2']:.4f}, Test RMSE: {tree_results['Test RMSE']:,.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top levels of the Decision Tree:\n",
      "|--- Duration (in Days) <= -1.32\n",
      "|   |--- Budget (in Billion $) <= -0.32\n",
      "|   |   |--- Country_UK <= 1.28\n",
      "|   |   |   |--- truncated branch of depth 8\n",
      "|   |   |--- Country_UK >  1.28\n",
      "|   |   |   |--- truncated branch of depth 7\n",
      "|   |--- Budget (in Billion $) >  -0.32\n",
      "|   |   |--- Budget (in Billion $) <= 1.37\n",
      "|   |   |   |--- truncated branch of depth 8\n",
      "|   |   |--- Budget (in Billion $) >  1.37\n",
      "|   |   |   |--- truncated branch of depth 8\n",
      "|--- Duration (in Days) >  -1.32\n",
      "|   |--- Duration (in Days) <= 0.48\n",
      "|   |   |--- Tech_Budget_Power <= 2.06\n",
      "|   |   |   |--- truncated branch of depth 8\n",
      "|   |   |--- Tech_Budget_Power >  2.06\n",
      "|   |   |   |--- truncated branch of depth 8\n",
      "|   |--- Duration (in Days) >  0.48\n",
      "|   |   |--- Year <= -1.48\n",
      "|   |   |   |--- truncated branch of depth 8\n",
      "|   |   |--- Year >  -1.48\n",
      "|   |   |   |--- truncated branch of depth 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import export_text #did a little googleing to see if i can get something a little show-y\n",
    "tree_rules = export_text(tree_trained, feature_names=list(X.columns), max_depth=2)\n",
    "print(\"\\nTop levels of the Decision Tree:\")\n",
    "print(tree_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model         Decission-Maker Regression Arts: 「Yggdrasil」\n",
      "Train R2                                          0.379376\n",
      "Test R2                                          -0.405995\n",
      "Train RMSE                                       11.719828\n",
      "Test RMSE                                        18.028037\n",
      "Train MAE                                         8.942275\n",
      "Test MAE                                         15.004797\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(tree_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok so, theres actuall a large gap between train and test  R^2. so a case of overfitting with this model. so that means finally the most optimal model i should do is Random Forest. (you knew this all along Abishek you sly dog! my heart was breaking out here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and evaluate a Random Forest model\n",
    "#\n",
    "# Create a RandomForestRegressor with: /// TIME FOR SOME ENSEMBLE LEARNING BAYBEEEEEEE\n",
    "#   - n_estimators=100\n",
    "#   - max_depth=10\n",
    "#   - random_state=42\n",
    "#   - n_jobs=-1 (use all CPU cores)  If I don't show up to class, let the class know my crappy laptop cooked off like an anti-tank mine \n",
    "#\n",
    "# Use evaluate_model() to train and evaluate\n",
    "# Add results to all_results and trained_models\n",
    "#### set up the random forest parameters first, Its not like the other models \n",
    "rf_model = RandomForestRegressor(   \n",
    "    n_estimators=100, \n",
    "    max_depth=10, \n",
    "    random_state=42, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"Random Forest - Test R²: {rf_results['Test R2']:.4f}, Test RMSE: {rf_results['Test RMSE']:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Your Own Model (Optional)\n",
    "\n",
    "*Feel free to try additional models or tune hyperparameters!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ADDITIONAL MODEL HERE:\n",
    "# Try GradientBoostingRegressor, different hyperparameters, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Cross-Validation (More Robust Evaluation)\n",
    "\n",
    "Cross-validation gives us a more reliable estimate of model performance by testing on multiple different train/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation on top models\n",
    "print(\"5-Fold Cross-Validation Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "models_to_cv = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "for name, model in models_to_cv.items():\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "    cv_results.append({\n",
    "        'Model': name,\n",
    "        'CV Mean R²': scores.mean(),\n",
    "        'CV Std R²': scores.std()\n",
    "    })\n",
    "    print(f\"{name}: R² = {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Cross-Validation Matters:**\n",
    "- A model that performs well on one train/test split might just be lucky\n",
    "- CV tests on 5 different splits, giving us confidence in the results\n",
    "- Lower standard deviation = more consistent/reliable model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.round(4)\n",
    "results_df = results_df.sort_values('Test R2', ascending=False)\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# R² Comparison\n",
    "models = results_df['Model']\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, results_df['Train R2'], width, label='Train R²', alpha=0.8)\n",
    "axes[0].bar(x + width/2, results_df['Test R2'], width, label='Test R²', alpha=0.8)\n",
    "axes[0].set_ylabel('R² Score')\n",
    "axes[0].set_title('R² Comparison Across Models')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1.1)\n",
    "axes[0].axhline(y=0.7, color='green', linestyle='--', alpha=0.5, label='Good threshold')\n",
    "\n",
    "# RMSE Comparison\n",
    "axes[1].bar(x - width/2, results_df['Train RMSE'], width, label='Train RMSE', alpha=0.8)\n",
    "axes[1].bar(x + width/2, results_df['Test RMSE'], width, label='Test RMSE', alpha=0.8)\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].set_title('RMSE Comparison Across Models')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Iteration Reflection\n",
    "\n",
    "*Before selecting your best model, reflect on what you learned:*\n",
    "\n",
    "**Questions to answer:**\n",
    "- Which model improved most over the baseline?\n",
    "- Did any models show signs of overfitting (big train vs test gap)?\n",
    "- Did regularization (Ridge/Lasso) help or hurt performance?\n",
    "- Did tree-based models (Decision Tree, Random Forest) work better than linear models?\n",
    "\n",
    "**Your reflection:**\n",
    "\n",
    "[Write your reflection here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Feature Importance & Selection\n",
    "\n",
    "**Important:** Your final model should use only **4-8 features**. This section helps you identify which features matter most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Random Forest (works well for this)\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_trained.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Random Forest):\")\n",
    "print(rf_importance)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(rf_importance['Feature'][::-1], rf_importance['Importance'][::-1])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance (Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also check correlations with target\n",
    "correlations = X_train.corrwith(y_train).abs().sort_values(ascending=False)\n",
    "print(\"Absolute Correlations with Target:\")\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select your top features (4-8 features)\n",
    "#\n",
    "# Based on the importance analysis above, choose your best features.\n",
    "# Consider both Random Forest importance AND correlations.\n",
    "# Also think about what makes sense from a domain perspective.\n",
    "\n",
    "SELECTED_FEATURES = [\n",
    "    # YOUR FEATURES HERE - add 4-8 feature names:\n",
    "    # 'feature1',\n",
    "    # 'feature2',\n",
    "    # etc.\n",
    "]\n",
    "\n",
    "# Fallback if you haven't selected yet\n",
    "if len(SELECTED_FEATURES) == 0:\n",
    "    SELECTED_FEATURES = rf_importance['Feature'].head(6).tolist()\n",
    "    print(f\"Using top 6 features from Random Forest: {SELECTED_FEATURES}\")\n",
    "else:\n",
    "    print(f\"Selected features ({len(SELECTED_FEATURES)}): {SELECTED_FEATURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain with selected features only\n",
    "X_train_selected = X_train_scaled[SELECTED_FEATURES]\n",
    "X_test_selected = X_test_scaled[SELECTED_FEATURES]\n",
    "\n",
    "print(f\"Training with {len(SELECTED_FEATURES)} selected features...\")\n",
    "\n",
    "# Test a few models with selected features\n",
    "selected_results = []\n",
    "\n",
    "for name, model in [('Linear Regression', LinearRegression()),\n",
    "                    ('Ridge', Ridge(alpha=1.0)),\n",
    "                    ('Random Forest', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42))]:\n",
    "    results, trained, _ = evaluate_model(model, X_train_selected, X_test_selected, y_train, y_test, name)\n",
    "    selected_results.append(results)\n",
    "    print(f\"{name} with {len(SELECTED_FEATURES)} features - Test R²: {results['Test R2']:.4f}\")\n",
    "\n",
    "selected_df = pd.DataFrame(selected_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Justification\n",
    "\n",
    "**Questions to answer:**\n",
    "- Which features did you select and why?\n",
    "- Did performance drop significantly with fewer features?\n",
    "- Do these features make sense from a domain perspective?\n",
    "- These are the features users will input in your Streamlit app—are they reasonable to ask for?\n",
    "\n",
    "**Your justification:**\n",
    "\n",
    "[Write your justification here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose and train your final model with selected features\n",
    "#\n",
    "# Based on your analysis, pick the best model type and instantiate it.\n",
    "# Consider: performance, consistency, simplicity\n",
    "#\n",
    "# Example: final_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# final_model = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate final model\n",
    "final_model.fit(X_train_selected, y_train)\n",
    "y_pred = final_model.predict(X_test_selected)\n",
    "\n",
    "# Final metrics\n",
    "final_r2 = r2_score(y_test, y_pred)\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "final_mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"FINAL MODEL PERFORMANCE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {type(final_model).__name__}\")\n",
    "print(f\"Features: {SELECTED_FEATURES}\")\n",
    "print(f\"\\nTest R²: {final_r2:.4f}\")\n",
    "print(f\"Test RMSE: {final_rmse:,.2f}\")\n",
    "print(f\"Test MAE: {final_mae:,.2f}\")\n",
    "print(f\"\\nRMSE as % of target range: {final_rmse/target_range*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Justification\n",
    "\n",
    "*Explain why you chose this model as your best:*\n",
    "\n",
    "**Questions to consider:**\n",
    "- Why did you select this model over others?\n",
    "- Is there significant overfitting (train vs test gap)?\n",
    "- How does the performance compare to your baseline?\n",
    "- Would a simpler model be almost as good?\n",
    "- Does the RMSE represent acceptable prediction error for your problem?\n",
    "\n",
    "**Your justification:**\n",
    "\n",
    "[Write your justification here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect prediction')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title(f'Actual vs Predicted ({type(final_model).__name__})')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residual distribution (should be roughly normal, centered at 0)\n",
    "axes[0].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Residual (Actual - Predicted)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Residuals')\n",
    "\n",
    "# Residuals vs Predicted (should show no pattern)\n",
    "axes[1].scatter(y_pred, residuals, alpha=0.5)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted Values')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].set_title('Residuals vs Predicted Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Residual mean (should be ~0): {residuals.mean():.2f}\")\n",
    "print(f\"Residual std: {residuals.std():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for final model (with selected features)\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    final_importance = pd.DataFrame({\n",
    "        'Feature': SELECTED_FEATURES,\n",
    "        'Importance': final_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(final_importance['Feature'], final_importance['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importance (Final Model)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "elif hasattr(final_model, 'coef_'):\n",
    "    final_importance = pd.DataFrame({\n",
    "        'Feature': SELECTED_FEATURES,\n",
    "        'Coefficient': final_model.coef_\n",
    "    }).sort_values('Coefficient', key=abs, ascending=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['green' if c > 0 else 'red' for c in final_importance['Coefficient']]\n",
    "    plt.barh(final_importance['Feature'], final_importance['Coefficient'], color=colors)\n",
    "    plt.xlabel('Coefficient')\n",
    "    plt.title('Feature Coefficients (Final Model)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12: Save Model and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new scaler fitted only on selected features\n",
    "final_scaler = StandardScaler()\n",
    "X_train_final = X_train[SELECTED_FEATURES]\n",
    "final_scaler.fit(X_train_final)\n",
    "\n",
    "# Save the best model\n",
    "model_path = '../models/regression_model.pkl'\n",
    "joblib.dump(final_model, model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Save the scaler (fitted on selected features only)\n",
    "scaler_path = '../models/regression_scaler.pkl'\n",
    "joblib.dump(final_scaler, scaler_path)\n",
    "print(f\"Scaler saved to {scaler_path}\")\n",
    "\n",
    "# Save feature names (the selected features for Streamlit app)\n",
    "features_path = '../models/regression_features.pkl'\n",
    "joblib.dump(SELECTED_FEATURES, features_path)\n",
    "print(f\"Features saved to {features_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify saved model works\n",
    "loaded_model = joblib.load(model_path)\n",
    "loaded_scaler = joblib.load(scaler_path)\n",
    "loaded_features = joblib.load(features_path)\n",
    "\n",
    "# Test prediction\n",
    "test_sample = X_test[loaded_features].iloc[[0]]\n",
    "test_sample_scaled = loaded_scaler.transform(test_sample)\n",
    "test_pred = loaded_model.predict(test_sample_scaled)\n",
    "\n",
    "print(f\"\\nModel verification:\")\n",
    "print(f\"Features used: {loaded_features}\")\n",
    "print(f\"Sample input: {test_sample.values[0]}\")\n",
    "print(f\"Predicted: {test_pred[0]:,.2f}\")\n",
    "print(f\"Actual: {y_test.iloc[0]:,.2f}\")\n",
    "print(f\"\\nModel saved and verified successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 13: Binning Strategy for Classification\n",
    "\n",
    "**IMPORTANT:** Before you start Notebook 03, you need to send your binning strategy to Abishek on Slack for approval by **Feb 15**.\n",
    "\n",
    "### Analyze Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at target distribution to help decide binning\n",
    "print(f\"Target: {TARGET}\")\n",
    "print(f\"\\nDescriptive Statistics:\")\n",
    "print(y.describe())\n",
    "\n",
    "print(f\"\\nPercentiles:\")\n",
    "for p in [10, 25, 33, 50, 67, 75, 90]:\n",
    "    print(f\"{p}th percentile: {y.quantile(p/100):,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize potential binning strategies\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram with quartile lines\n",
    "axes[0].hist(y, bins=30, edgecolor='black', alpha=0.7)\n",
    "for p, color, style in [(25, 'orange', '--'), (50, 'red', '-'), (75, 'orange', '--')]:\n",
    "    axes[0].axvline(y.quantile(p/100), color=color, linestyle=style, linewidth=2,\n",
    "                     label=f'{p}th percentile: {y.quantile(p/100):,.0f}')\n",
    "axes[0].set_xlabel(TARGET)\n",
    "axes[0].set_title('Distribution with Quartile Lines')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(y)\n",
    "axes[1].set_ylabel(TARGET)\n",
    "axes[1].set_title('Box Plot of Target')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed Binning Strategy\n",
    "\n",
    "*Complete this section and send to Abishek on Slack by Feb 15:*\n",
    "\n",
    "**1. How many categories will you create?**\n",
    "\n",
    "[Your answer - e.g., 3 categories: Low, Medium, High]\n",
    "\n",
    "**2. What are your bin thresholds?**\n",
    "\n",
    "[Your answer - e.g., Low: < $30,000, Medium: $30,000-$60,000, High: > $60,000]\n",
    "\n",
    "**3. Why does this binning make sense for your problem?**\n",
    "\n",
    "[Your answer - explain the domain reasoning. Why are these meaningful categories?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview your binning\n",
    "def create_bins_preview(y, strategy='quartile'):\n",
    "    if strategy == 'quartile':\n",
    "        bins = [y.min()-1, y.quantile(0.25), y.quantile(0.75), y.max()+1]\n",
    "        labels = ['Low', 'Medium', 'High']\n",
    "    elif strategy == 'tertile':\n",
    "        bins = [y.min()-1, y.quantile(0.33), y.quantile(0.67), y.max()+1]\n",
    "        labels = ['Low', 'Medium', 'High']\n",
    "    # Add your custom strategy here if needed\n",
    "    \n",
    "    return pd.cut(y, bins=bins, labels=labels)\n",
    "\n",
    "# Preview with quartile binning\n",
    "y_binned = create_bins_preview(y, 'quartile')\n",
    "print(\"Preview of binned target (using quartiles):\")\n",
    "print(y_binned.value_counts().sort_index())\n",
    "print(f\"\\nPercentages:\")\n",
    "print((y_binned.value_counts(normalize=True) * 100).round(1).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What You Accomplished\n",
    "- [ ] Loaded and prepared cleaned data\n",
    "- [ ] Split data into train and test sets\n",
    "- [ ] Scaled features appropriately\n",
    "- [ ] Built a baseline model\n",
    "- [ ] Tried multiple model types\n",
    "- [ ] Performed cross-validation\n",
    "- [ ] Selected top 4-8 features\n",
    "- [ ] Compared and selected best model\n",
    "- [ ] Analyzed model performance (residuals, feature importance)\n",
    "- [ ] Saved model, scaler, and feature list\n",
    "- [ ] Planned binning strategy for classification\n",
    "\n",
    "### Key Results\n",
    "\n",
    "**Best Model:** [Model name]\n",
    "\n",
    "**Selected Features:** [List them]\n",
    "\n",
    "**Test R²:** [Value]\n",
    "\n",
    "**Test RMSE:** [Value]\n",
    "\n",
    "**Improvement over baseline:** [Percentage or description]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint 3 Submission Instructions\n",
    "\n",
    "**Congratulations!** You've completed Checkpoint 3 (Regression Model).\n",
    "\n",
    "### Step 1: Save This Notebook\n",
    "- File -> Save (or Ctrl+S / Cmd+S)\n",
    "\n",
    "### Step 2: Send Binning Strategy to Abishek\n",
    "- Message Abishek on Slack with your binning strategy\n",
    "- Include: number of categories, thresholds, and justification\n",
    "\n",
    "### Step 3: Commit to GitHub\n",
    " !!!!!!!!!!!!!! I have been opening my gitbash directy in my capstone project folder an pushing stuff. just using regular git commands was not working. I think its the same with the data and the models. I'm not sure how to get it right!!!!!!\n",
    "```bash\n",
    "# Stage your changes\n",
    "git add notebooks/02_regression_model.ipynb\n",
    "git add models/\n",
    "\n",
    "# Commit with a meaningful message\n",
    "git commit -m \"Complete Checkpoint 3: Regression model with feature selection\"\n",
    "\n",
    "# Push to GitHub\n",
    "git push\n",
    "```\n",
    "\n",
    "### Step 4: Submit to Canvas\n",
    "1. Go to the Checkpoint 3 assignment on Canvas\n",
    "2. Submit the link to your GitHub repository\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Wait for binning approval** from Abishek\n",
    "2. Move on to **Notebook 03: Classification Model**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
